{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLth29+Ot1+tfpdlx3F5gY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2b10ad6d55140fcaad83fc04fbd01df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dc8f8f3c236434398269b1cc82168d8",
              "IPY_MODEL_16c318b5410d4171a1755a86862d8001"
            ],
            "layout": "IPY_MODEL_e1a3e7503837468cb4c4a3d4d0888598"
          }
        },
        "1dc8f8f3c236434398269b1cc82168d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5de811a8024a4f9f528deeb8720aea",
            "placeholder": "​",
            "style": "IPY_MODEL_b8a7c4e84c514ecc8ec1a82e31403df0",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "16c318b5410d4171a1755a86862d8001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af596548e8b24e638c1fababcf88a704",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccbf6c888ed244a792c3e5bb377b65a6",
            "value": 1
          }
        },
        "e1a3e7503837468cb4c4a3d4d0888598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5de811a8024a4f9f528deeb8720aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8a7c4e84c514ecc8ec1a82e31403df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af596548e8b24e638c1fababcf88a704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbf6c888ed244a792c3e5bb377b65a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asha629399/DL_ASSIGNMENT_1_CS22M021/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_M7HMhbolDP",
        "outputId": "9d661fa7-e293-471f-b38d-0d1f88952b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.21.1-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=df54ff62d5b06ffb86d4b86cf24282b91222346a08754e9d88729d40d5691848\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.21.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colour\n",
            "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: colour\n",
            "Successfully installed colour-0.1.5\n"
          ]
        }
      ],
      "source": [
        "# Install wandb and wordCloud libraries\n",
        "!pip install wandb\n",
        "!pip install wordcloud\n",
        "!pip install colour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrP47Zi3KQ60"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from math import log\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import tensorflow \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wandb and wandbcallback libraries\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "z698XKbDASMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOxnglCm3_jB"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import Counter\n",
        "from colour import Color\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMto8GM44Hzz",
        "outputId": "b7dc519c-8dc7-402b-e557-67bbed2cfd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# open the CSV file using csv.reader\n",
        "with open('/content/drive/MyDrive/hin_train.csv', newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
        "    data = list(reader)\n",
        "\n",
        "# convert the data to a pandas DataFrame\n",
        "train_file = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "# save as an XLSX file\n",
        "train_file=train_file.to_excel('/content/drive/MyDrive/hin_train.xlsx', index=False)\n",
        "\n",
        "\n",
        "# Load the validation dataset file\n",
        "with open('/content/drive/MyDrive/hin_valid.csv', newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
        "    data = list(reader)\n",
        "\n",
        "# convert the data to a pandas DataFrame\n",
        "val_file = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "# save as an XLSX file\n",
        "val_file=val_file.to_excel('/content/drive/MyDrive/hin_valid.xlsx', index=False)\n",
        "\n",
        "with open('/content/drive/MyDrive/hin_test.csv', newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
        "    data = list(reader)\n",
        "\n",
        "# convert the data to a pandas DataFrame\n",
        "test_file = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "# save as an XLSX file\n",
        "test_file=test_file.to_excel('/content/drive/MyDrive/hin_test.xlsx', index=False)\n",
        "\n",
        "# Path to save the predictions file\n",
        "\n",
        "# Create a folder named \"MyModelPredictions\" in the root directory of your Google Drive\n",
        "!mkdir \"/content/drive/MyDrive/MyModelPredictions\"\n",
        "\n",
        "# = '/content/drive/MyDrive/MyModelPredictions/predictions.tsv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBS9SCQy4P9H",
        "outputId": "af8873d8-ac26-4d88-96ee-78c663fc2f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/MyModelPredictions’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read train file into a pandas dataframe\n",
        "train_file = '/content/drive/MyDrive/hin_train.xlsx'\n",
        "train_df = pd.read_excel(train_file)\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "lines = train_df.to_csv(index=False, header=None, sep=\"\\t\").split(\"\\n\")\n",
        "\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text = line.split(\"\\t\")  # remove the third variable\n",
        "\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "     \n"
      ],
      "metadata": {
        "id": "_DtJYRDq5gfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total list of input words(input)\n",
        "total_input_words = input_texts + val_input_texts + test_input_texts\n",
        "\n",
        "# Get the total list of target words(target)\n",
        "total_target_words = target_texts + val_target_texts + test_target_texts"
      ],
      "metadata": {
        "id": "Stu4C_Wt8e3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "val_file = '/content/drive/MyDrive/hin_valid.xlsx'\n",
        "val_df = pd.read_excel(val_file)\n",
        "\n",
        "lines = val_df.to_csv(index=False, header=None, sep=\"\\t\").split(\"\\n\")\n",
        "    \n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    val_input_texts.append(input_text)\n",
        "    val_target_texts.append(target_text)"
      ],
      "metadata": {
        "id": "qzknHM3K8lFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "test_file = '/content/drive/MyDrive/hin_test.xlsx'\n",
        "test_df = pd.read_excel(test_file)\n",
        "\n",
        "lines = val_df.to_csv(index=False, header=None, sep=\"\\t\").split(\"\\n\")\n",
        "\n",
        "for line in lines[: len(lines) - 1]:\n",
        "    target_text, input_text = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    test_input_texts.append(input_text)\n",
        "    test_target_texts.append(target_text)"
      ],
      "metadata": {
        "id": "xx_ny435hHy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list of input characters(input) and target characters(target)\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Get total number of characters in Input \n",
        "num_encoder_tokens = len(input_characters)\n",
        "# Get total number of characters in Output \n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "# Get maximum length of the word in total input words\n",
        "max_encoder_seq_length = max([len(text) for text in total_input_words])\n",
        "# Get maximum length of the word in total target words\n",
        "max_decoder_seq_length = max([len(text) for text in total_target_words])\n",
        "\n",
        "print(\"Summary of the dataset :\")\n",
        "print(\"Number of train samples :\" , len(input_texts))\n",
        "print(\"Number of val samples :\" , len(val_input_texts))\n",
        "print(\"Number of test samples :\" , len(test_input_texts))\n",
        "print(\"Number of unique input tokens :\" , num_encoder_tokens)\n",
        "print(\"Number of unique output tokens :\" , num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\" , max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\" , max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R-GdbZ59IW6",
        "outputId": "7ed2d779-e92a-4844-d54c-92d947db5cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the dataset :\n",
            "Number of train samples : 51199\n",
            "Number of val samples : 4095\n",
            "Number of test samples : 4095\n",
            "Number of unique input tokens : 65\n",
            "Number of unique output tokens : 29\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for every input character with an index associated to it\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "# Dictionary for every target character with an index associated to it\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# Dictionary for every input character with an index associated to it\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Dictionary for every input character with an index associated to it\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Preparing train encoder and decoder inputs and decoder target\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Writing the encoder , decoder characters with the indexes in input and target token index \n",
        "for i, (input, target) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(target):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the decoder_target_data  and decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    \n",
        "    decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "qr1F-7KZ9pZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for every input character with an index associated to it\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "# Dictionary for every target character with an index associated to it\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# Dictionary for every input character with an index associated to it\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Dictionary for every input character with an index associated to it\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "# Preparing train encoder and decoder inputs and decoder target\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "# Writing the encoder , decoder characters with the indexes in input and target token index \n",
        "for i, (input, target) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(target):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the decoder_target_data  and decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    \n",
        "    decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "MXUpDNvr9qU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "val_encoder_input_data = np.zeros((len(val_input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "val_decoder_input_data = np.zeros((len(val_input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "val_decoder_target_data = np.zeros((len(val_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, hindi) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(english):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(hindi):\n",
        "        val_decoder_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    val_decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    val_decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "ZxvG3A4d9wQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing test encoder and decoder inputs\n",
        "\n",
        "test_encoder_input_data = np.zeros((len(test_input_texts), max_encoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_input_data = np.zeros((len(test_input_texts), max_decoder_seq_length), dtype=\"float32\")\n",
        "test_decoder_target_data = np.zeros((len(test_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input, target) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input):\n",
        "        test_encoder_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(target):\n",
        "        test_decoder_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # One got encoding the validation decoder target data  and validation decoder target data will be ahead by one timestep and will not include the start character.\n",
        "            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0   \n",
        "    test_decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "    test_decoder_target_data[i, t :, target_token_index[' ']] = 1.0"
      ],
      "metadata": {
        "id": "3GlyGUtqcmKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the validation target and input words to nupy arrays\n",
        "val_target_texts = np.array(val_target_texts)\n",
        "val_input_texts = np.array(val_input_texts)\n",
        "\n",
        "# Converting the test target and input words to numpy arrays\n",
        "test_target_texts = np.array(test_target_texts)\n",
        "test_input_texts = np.array(test_input_texts)"
      ],
      "metadata": {
        "id": "ThdDSJzmc6te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(predictions_path, 'w') as f:\n",
        " #   f.write('Input input Words, Predicted target Words, Actual target Words\\n')"
      ],
      "metadata": {
        "id": "l1AFZgv4-Pzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_model(epochs, latent_dim, batch_size, optimizer_fn, dropout, recc_dropout):\n",
        "    encoder = nn.RNN(num_encoder_tokens, latent_dim)\n",
        "    decoder = nn.RNN(num_decoder_tokens, latent_dim)\n",
        "\n",
        "    encoder_model = EncoderModel(encoder)\n",
        "    decoder_model = DecoderModel(decoder)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optimizer_fn(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(encoder_input_data), batch_size):\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            encoder_hidden = encoder_model(encoder_input_data[i:i+batch_size])\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_input = torch.zeros((batch_size, 1, num_decoder_tokens)).to(device)\n",
        "            decoder_outputs = []\n",
        "            for timestep in range(decoder_target_data.shape[1]):\n",
        "                decoder_output, decoder_hidden = decoder_model(decoder_input, decoder_hidden)\n",
        "                decoder_outputs.append(decoder_output)\n",
        "                decoder_input = decoder_target_data[i:i+batch_size, timestep:timestep+1, :]\n",
        "            decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "            loss = loss_fn(decoder_outputs.permute(0, 2, 1), decoder_target_data[i:i+batch_size].squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # validation loss\n",
        "        val_encoder_hidden = encoder_model(val_encoder_input_data)\n",
        "        val_decoder_hidden = val_encoder_hidden\n",
        "        val_decoder_input = torch.zeros((len(val_encoder_input_data), 1, num_decoder_tokens)).to(device)\n",
        "        val_decoder_outputs = []\n",
        "        for timestep in range(val_decoder_target_data.shape[1]):\n",
        "            val_decoder_output, val_decoder_hidden = decoder_model(val_decoder_input, val_decoder_hidden)\n",
        "            val_decoder_outputs.append(val_decoder_output)\n",
        "            val_decoder_input = val_decoder_target_data[:, timestep:timestep+1, :].to(device)\n",
        "        val_decoder_outputs = torch.cat(val_decoder_outputs, dim=1)\n",
        "        val_loss = loss_fn(val_decoder_outputs.permute(0, 2, 1), val_decoder_target_data.squeeze().to(device))\n",
        "\n",
        "        print(\"Epoch:\", epoch, \"Train Loss:\", loss.item(), \"Val Loss:\", val_loss.item())\n",
        "\n",
        "    return model, encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "0pLwBWXCBYcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(EncoderModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, input):\n",
        "        output, hidden = self.encoder(input)\n",
        "        return hidden\n",
        "\n",
        "class DecoderModel(nn.Module):\n",
        "    def __init__(self, decoder):\n",
        "        super(DecoderModel, self).__init__()\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.decoder(input, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        encoder_hidden = self.encoder(encoder_input)\n",
        "        decoder_output, _ = self.decoder(decoder_input, encoder_hidden)\n",
        "        return decoder_output\n",
        "\n",
        "def GRU_model(epochs, latent_dim, batch_size, optimizer_fn, dropout, recc_dropout):\n",
        "    \n",
        "\n",
        "    \n",
        "    encoder = nn.GRU(num_encoder_tokens, latent_dim)\n",
        "    decoder = nn.GRU(num_decoder_tokens, latent_dim)\n",
        "\n",
        "    encoder_model = EncoderModel(encoder)\n",
        "    decoder_model = DecoderModel(decoder)\n",
        "\n",
        "    model = Seq2Seq(encoder, decoder).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optimizer_fn(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(encoder_input_data), batch_size):\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            encoder_hidden = encoder_model(encoder_input_data[i:i+batch_size])\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_input = torch.zeros((batch_size, 1, num_decoder_tokens)).to(device)\n",
        "            decoder_outputs = []\n",
        "            for timestep in range(decoder_target_data.shape[1]):\n",
        "                decoder_output, decoder_hidden = decoder_model(decoder_input, decoder_hidden)\n",
        "                decoder_outputs.append(decoder_output)\n",
        "                decoder_input = decoder_target_data[i:i+batch_size, timestep:timestep+1, :]\n",
        "            decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "            loss = loss_fn(decoder_outputs.permute(0, 2, 1), decoder_target_data[i:i+batch_size].squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # validation loss\n",
        "        val_encoder_hidden = encoder_model(val_encoder_input_data)\n",
        "        val_decoder_hidden = val_encoder_hidden\n",
        "        val_decoder_input = torch.zeros((len(val_encoder_input_data), 1, num_decoder_tokens)).to(device)\n",
        "        val_decoder_outputs = []\n",
        "        for timestep in range(val_decoder_target_data.shape[1]):\n",
        "            val_decoder_output, val_decoder_hidden = decoder_model(val_decoder_input, val_decoder_hidden)\n",
        "            val_decoder_outputs.append(val_decoder_output)\n",
        "            val_decoder_input = val_decoder_target_data[:, timestep:timestep+1, :].to(device)\n",
        "        val_decoder_outputs = torch.cat(val_decoder_outputs, dim=1)\n",
        "        val_loss = loss_fn(val_decoder_outputs.permute(0, 2, 1), val_decoder_target_data.squeeze().to(device))\n",
        "\n",
        "        print(\"Epoch:\", epoch, \"Train Loss:\", loss.item(), \"Val Loss:\", val_loss.item())\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ],
      "metadata": {
        "id": "Pw34bM-TGFQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(num_encoder_tokens, latent_dim, batch_first=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        _, (hidden, cell) = self.encoder(x)\n",
        "        return hidden, cell\n",
        "\n",
        "class DecoderModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.LSTM(num_decoder_tokens, latent_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(latent_dim, num_decoder_tokens)\n",
        "        \n",
        "    def forward(self, x, hidden, cell):\n",
        "        output, (hidden, cell) = self.decoder(x, (hidden, cell))\n",
        "        output = self.fc(output)\n",
        "        return output, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        encoder_hidden, encoder_cell = self.encoder(encoder_input)\n",
        "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
        "        decoder_outputs = []\n",
        "        for i in range(decoder_input.shape[1]):\n",
        "            output, decoder_hidden, decoder_cell = self.decoder(decoder_input[:, i:i+1, :], decoder_hidden, decoder_cell)\n",
        "            decoder_outputs.append(output)\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        return decoder_outputs\n",
        "\n",
        "def LSTM_model(epochs, latent_dim, batch_size, optimizer_fn, dropout, recc_dropout):\n",
        "    encoder_model = EncoderModel(latent_dim)\n",
        "    decoder_model = DecoderModel(latent_dim)\n",
        "    model = Seq2Seq(encoder_model, decoder_model).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optimizer_fn(model.parameters())\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(encoder_input_data), batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            encoder_input_batch = encoder_input_data[i:i+batch_size]\n",
        "            decoder_input_batch = decoder_input_data[i:i+batch_size]\n",
        "            decoder_target_batch = decoder_target_data[i:i+batch_size]\n",
        "            output = model(encoder_input_batch, decoder_input_batch)\n",
        "            loss = loss_fn(output.permute(0, 2, 1), decoder_target_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            val_output = model(val_encoder_input_data, val_decoder_input_data)\n",
        "            val_loss = loss_fn(val_output.permute(0, 2, 1), val_decoder_target_data)\n",
        "            \n",
        "        print(\"Epoch:\", epoch, \"Train Loss:\", loss.item(), \"Val Loss:\", val_loss.item())\n",
        "        \n",
        "    return model, encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "Z5x8oyKnJfQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = encoder_model(input_seq)\n",
        "\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = torch.zeros((1, 1, num_decoder_tokens))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "        # Sampling loop for a batch of sequences\n",
        "        # (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            decoder_outputs, hidden = decoder_model(target_seq, hidden)\n",
        "            # Sample a token\n",
        "            sampled_token_index = decoder_outputs.argmax(dim=2).item()\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = torch.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Convert the decoded sentence from a tensor to a string\n",
        "        decoded_sentence = decoded_sentence.replace(\"\\n\", \"\")\n",
        "        return decoded_sentence"
      ],
      "metadata": {
        "id": "NS_tTNWFKkWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def decode_sequence_lstm(input_seq, encoder_model, decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    input_seq = torch.from_numpy(input_seq).float()\n",
        "    states_value = encoder_model(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = torch.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        target_seq = target_seq.to(device)\n",
        "        states_value = [state.to(device) for state in states_value]\n",
        "        output_tokens, h, c = decoder_model(target_seq, states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        output_tokens = output_tokens.squeeze(1)\n",
        "        sampled_token_index = output_tokens.argmax(dim=-1)\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = torch.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        h = h.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "VbyeCzCQW4IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Run(epochs,latent_dim, batch_size, dropout, recc_dropout, cell_type, optimizer_fn):\n",
        "  if cell_type=='LSTM':\n",
        "    model,encoder_model,decoder_model = LSTM_model(epochs,latent_dim, batch_size, optimizer_fn, dropout, recc_dropout)\n",
        "  elif cell_type=='GRU':\n",
        "    model,encoder_model,decoder_model = GRU_model(epochs,latent_dim, batch_size, optimizer_fn, dropout, recc_dropout)\n",
        "  elif cell_type=='RNN':\n",
        "    model,encoder_model,decoder_model = RNN_model(epochs,latent_dim, batch_size, optimizer_fn, dropout, recc_dropout)\n",
        "  valid = 0\n",
        "  samples=1000\n",
        "  for i in range(samples):\n",
        "      seq_index=random.randint(0,len(test_input_texts))\n",
        "      input_seq = test_encoder_input_data[seq_index : seq_index + 1]\n",
        "      if cell_type=='LSTM':\n",
        "        decoded_sentence = decode_sequence_lstm(input_seq,encoder_model,decoder_model)\n",
        "      else:\n",
        "        decoded_sentence = decode_sequence(input_seq,encoder_model,decoder_model)\n",
        "      flag=1\n",
        "      for (i,j) in zip(decoded_sentence,test_target_texts[seq_index]):\n",
        "        if i!=j:\n",
        "          flag=0\n",
        "          break\n",
        "      if flag==1:\n",
        "        valid += 1\n",
        "  return (valid/samples)*100"
      ],
      "metadata": {
        "id": "p2yQmPn4XRMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'model_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epoch': {\n",
        "            'values': [40]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64]\n",
        "        },\n",
        "        'dropout':{\n",
        "            'values': [0.1,0.01,0.0]\n",
        "        },\n",
        "        'recc_dropout':{\n",
        "            'values': [0.1,0.01,0.0]\n",
        "        },\n",
        "        'latent_dim':{\n",
        "            'values': [64,126,256]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values':['LSTM','RNN','GRU']\n",
        "        },\n",
        "        'optimizer_fn': {\n",
        "            'values': ['adam','rmsprop']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ZUUuM-o7XWk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config,project=\"DL_Assignment_3a\")\n",
        "#c467a6693ff6ce5eff3b78a68a9fd6bed4d726cd"
      ],
      "metadata": {
        "id": "x4SIYrmsXcME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "0ebde4b3-8ea2-42e5-b07b-4184d21bf3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: t2h4ml80\n",
            "Sweep URL: https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    config_defaults={\n",
        "        'epoch':10,\n",
        "        'batch_size':64,\n",
        "        'dropout':0.01,\n",
        "        'recc_dropout':0.1,\n",
        "        'cell_type':'LSTM',\n",
        "        'optimizer_fn':'rmsprop',\n",
        "        }\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    model_acc=Run(config.epoch,config.latent_dim, config.batch_size,config.dropout, config.recc_dropout, config.cell_type, config.optimizer_fn)  \n",
        "    wandb.log({'model_accuracy': model_acc})\n",
        "    return\n",
        "     "
      ],
      "metadata": {
        "id": "bSXmP1AMXrgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train,count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c2b10ad6d55140fcaad83fc04fbd01df",
            "1dc8f8f3c236434398269b1cc82168d8",
            "16c318b5410d4171a1755a86862d8001",
            "e1a3e7503837468cb4c4a3d4d0888598",
            "ed5de811a8024a4f9f528deeb8720aea",
            "b8a7c4e84c514ecc8ec1a82e31403df0",
            "af596548e8b24e638c1fababcf88a704",
            "ccbf6c888ed244a792c3e5bb377b65a6"
          ]
        },
        "id": "8NROAZ3KXure",
        "outputId": "c1bde4d5-7702-4fd7-b40a-32be7ddbdbd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h11laph4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_fn: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecc_dropout: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230501_085414-h11laph4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/h11laph4' target=\"_blank\">lively-sweep-10</a></strong> to <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/h11laph4' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/h11laph4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lively-sweep-10</strong> at: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/h11laph4' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/h11laph4</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230501_085414-h11laph4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run h11laph4 errored: TypeError(\"'str' object is not callable\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run h11laph4 errored: TypeError(\"'str' object is not callable\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m8dze1ut with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_fn: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecc_dropout: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669554666668772, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2b10ad6d55140fcaad83fc04fbd01df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230501_085423-m8dze1ut</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/m8dze1ut' target=\"_blank\">stellar-sweep-11</a></strong> to <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/m8dze1ut' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/m8dze1ut</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-sweep-11</strong> at: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/m8dze1ut' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/m8dze1ut</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230501_085423-m8dze1ut/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run m8dze1ut errored: TypeError(\"'str' object is not callable\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run m8dze1ut errored: TypeError(\"'str' object is not callable\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 89reecqf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_fn: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trecc_dropout: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230501_085433-89reecqf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/89reecqf' target=\"_blank\">feasible-sweep-12</a></strong> to <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/sweeps/t2h4ml80</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/89reecqf' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/89reecqf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-sweep-12</strong> at: <a href='https://wandb.ai/cs22m021/DL_Assignment_3a/runs/89reecqf' target=\"_blank\">https://wandb.ai/cs22m021/DL_Assignment_3a/runs/89reecqf</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230501_085433-89reecqf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Run 89reecqf errored: TypeError(\"'str' object is not callable\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 89reecqf errored: TypeError(\"'str' object is not callable\")\n",
            "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
          ]
        }
      ]
    }
  ]
}